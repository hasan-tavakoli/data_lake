Approach to a Data Lake:
One of the key principles is the choice of an approach to meet the requirements of ingesting, processing & consuming large volumes of data and cater to both near-real-time & batch mode operations. The obvious choices for this are – a traditional Data Warehouse, Data Marts or Data Lakes.
Data Lakes – data lakes are implemented with the principle of allowing any and all data in the enterprise to be ingested, explored, analysed and reported with minimal lag. The principle here is, the processing is driven by the use cases and to make data available to end-users and use cases as soon as available. This is achieved by:
 analysing use cases and creating specific ingestion & analysis jobs (specific to near real-time or batch use cases),
 allow structured, semi-structure & unstructured data be ingested, processed and analysed,
 enable exploratory data analysis & consumption,
 allow self-service data consume

Data Zones:
Each zone is a logic area used for coarse-grained access control & organization of data tables within a data lake. Further, the tables in a zone can be identified using tags.
Directory Structure for the Zones
Each Data zone shall have a dedicated directory created under the “Data Lake (Project name)” directory which in turn is created under the Big Query storage mount point created for the Data Lake.
The Datasets named – “Raw Data Zone”, “Trusted Data Zone”, “Processed Data Zone” are created. Further, Tables are to be created in the “Raw Data Zone” dataset as described in the section below.
For Trusted & Processed Data zones, tables under the zone directories (datasets) are to be created corresponding to the specific ingestion or wrangling jobs. The table names shall be the same as the job names for which the directory was created.
1-Raw Data Zone:
As the name suggests this zone is used to maintain copies / replicas of the raw data that is ingested into the Data Lake. The primary purpose of this zone is to retain raw data for a specified period, so that data maybe reprocessed on any use ace include data science tasks, re ingestion of data with new structure, mistake or error in ingestion to trusted zone or referred for auditing or debug purposes.
Raw Data Zone Directory Structure
Date, this is the date of collection or processing of the raw data from the source, typically named in the YYYYMMDD format. This is the final directory under which a successfully collected file is stored.
2-Trusted Zone:
RAW data ingested first into RAW data zone and after processing and normalization with Big query processors and custom processors ingested into Data Lake and lands into this trusted zone.
<Project Name>
└ Raw Data Zone (Dataset)
└ Trusted Data Zone (Dataset)
└ Processed Data Zone (Dataset)
Typical directory structure for zones
Subject Area
└ Date <YYYYMMDD>
Trusted Data Zone Directory Structure
The data that is moved to the Trusted Data Zone shall be typically validated to ensure data qualit.
3-Processed Data Zone:
All the output of data wrangling & analytics tasks primarily stored is this zone. This includes summaries, features or aggregates generated from input data in the trusted zone and data generated by various analytics models & algorithms run on the Data Lake.
